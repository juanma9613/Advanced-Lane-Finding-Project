{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReadMe\n",
    "### All the code and the required classes and functions are defined in the notebook: \"./examples/Project_advanced_line_detection.ipynb\" \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Calibration\n",
    "\n",
    "#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.\n",
    "\n",
    "The code for this step is contained in the first and second code cells of the IPython notebook located in \"./examples/Project_advanced_line_detection.ipynb\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some calibration chessboard images were provided in the repository with 9x6 inner corners.\n",
    "\n",
    "    -The corners coordinates in the pictures were found using the cv2.findChessboardCorners() function\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (9,6),None)\n",
    "    -And the real coordinates of the chessboard were generated assuming assuming squares of side = 1\n",
    "        objp = np.zeros((6*9,3), np.float32)\n",
    "        objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "        \n",
    "With the lists containing the object points and the image points, the distortion coefficients and the transformation matrix were calculated using opencv functions:\n",
    "\n",
    "    -ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, (w,h),None,None)\n",
    "\n",
    "The undistorted image was calculated using mtx and dist.\n",
    "    \n",
    "    -undist = cv2.undistort(img, mtx, dist,None,mtx)\n",
    "    \n",
    "\n",
    "Below are the initial image, and the undistorted result,it can be seen that the radial distortion has dissapeared in the bottom edge of the chessboard:\n",
    "##### Calibration image    \n",
    "![alt text](./camera_cal/calibration3.jpg \"cal image\")\n",
    "##### Undistorted image\n",
    "![alt text](./calibrated_chessboard/calibrated13.png \"undist image\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline (single images)\n",
    "\n",
    "The test image #2 that is below will be used for this demonstration:\n",
    "##### Frame original image\n",
    "![alt text](./test_images/test2.jpg \"framer image\")\n",
    "\n",
    "#### 1. Provide an example of a distortion-corrected image.\n",
    "Using the camera calibration mtx and the dist coefficients, the undistorted image is generated and can be seen below, the radial distortion of the horizontal lines on the road is corrected.\n",
    "    \n",
    "    -undist = cv2.undistort(img, mtx, dist,None,mtx)\n",
    "\n",
    "    \n",
    "##### Frame undistorted image, \n",
    "![alt text](./undist_test_images/undist2.png \"undistr image\")\n",
    "\n",
    "#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.\n",
    "\n",
    "For this step a pipeline function was defined in the third code cell of the notebook, it uses a combination of color and gradient thresholds to generate a binary image.  \n",
    "\n",
    "The filters used and their low and high threshold values for the filters are:\n",
    "\n",
    "    -1) S channel of HLS   : (170, 255)\n",
    "    -2) Sobel X filter     : (20, 100)\n",
    "    -3) Red channel of RGB : (200, 255)\n",
    "    -4) Hue channel of HLS : (15, 100)\n",
    " Then, the filter 3 and the 4 (Red and Hue) and intersected to form the filter 5\n",
    "     \n",
    "     -5) (3) Red AND (4) Hue\n",
    " \n",
    " The output is the union of the images from the filters: (1), (2) and  (5) using a logic or in python.\n",
    "     \n",
    " \n",
    " Here's an example of my output:\n",
    "\n",
    "    \n",
    "\n",
    "##### Frame thresholded image\n",
    "![alt text](./thresholded_images/thresholded2.png \"thresholdedr image\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.\n",
    "\n",
    "\n",
    "The code defining the warp perspective starts from the sixth cell of code in the notebook, after the title \"Applying the warp perspective\"\n",
    "\n",
    "These are the percentages used to get the corners of the trapezoidal region:\n",
    "\n",
    "bottom_y_percentage=0.935\n",
    "\n",
    "top_y_percentage=0.62\n",
    "\n",
    "width_x_top=0.06\n",
    "\n",
    "width_x_bottom=0.60\n",
    "\n",
    "I chose the hardcode the source and destination points in the following manner:\n",
    "\n",
    "```python\n",
    "offset=0.2*w\n",
    "src_points= np.float32([[w*(0.5-width_x_bottom/2),bottom_y_percentage*h],[w*(0.5+width_x_bottom/2),bottom_y_percentage*h],[w*(0.5+width_x_top/2),top_y_percentage*h],[w*(0.5-width_x_top/2),top_y_percentage*h]])\n",
    "dst_points= np.float32([[offset,h],[w-offset,h],[w-offset,0],[offset,0]])\n",
    "```\n",
    "\n",
    "I verified that my perspective transform was working as expected by warping the straight_lines images provided, and verifying that the warped lines were close to the vertical\n",
    "\n",
    "##### Thresholded image\n",
    "![alt text](./warped_straight/straight_undist0.png \"straight_lines image\")\n",
    "##### Warped image\n",
    "![alt text](./warped_straight/straight_warped0.png \"straight_lines image\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?\n",
    "\n",
    "After the title \"Definition of  the finding lane functions\" in the notebook\" I defined two functions:\n",
    "\n",
    "find_lane_pixels(): It executes the sliding window search of line's pixels and return the coordinates of all the pixels that are inside the windows.\n",
    "    \n",
    "    In this function, first the histogram of the bottom half of the image is generated, and the windows start in those coordinates, then, the windows move upwards, and the x coordinate is updated according to the centroid of the pixels inside each windows.\n",
    "\n",
    "fit_polynomial(): It fits a quadratic curve using the function of numpy polyfit. fit_line = np.polyfit(y_pixels,x_pixels , 2). the order of the arguments means that the function is x=f(y), it is because the curve is near the vertical so a vertical line may intersect the function in more than one point.\n",
    "\n",
    "My result can be seen below, where the blue and red windows are the left and right windows given by find_lane_pixels and the yelow curve is the curve fitted with the fit_polynomial function.\n",
    "\n",
    "The whole output video with the sliding windows is included in the project with the name \"./output_video_sliding_window.mp4\", here's a [link to this video](./output_video_sliding_window.mp4)\n",
    "\n",
    "\n",
    "\n",
    "![alt text](./sliding_window/sliding.png \"sliding window image\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.\n",
    "\n",
    "I calculated the radious of curvature inside the class called Line() it owns the calculate_curvature_and_position_and_newPoli(self,polynomial) methods wich receives the polynomial coefficients in pixel coordinates, and calculates the new coefficients in metters using the metters to pixel conversions (xm_per_pix and ym_per_pix) which were derived assuming the length between the lane lines and the length of\n",
    "\n",
    "ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    \n",
    "Then, the curvature function is calculated, and finally the curvature and position of each line are evaluated at the bottom of the picture.\n",
    "``` python\n",
    "    def calculate_curvature_and_position_and_newPoli(self,polynomial):\n",
    "        y_eval=self.ysize*self.ym_per_pix\n",
    "        Am=polynomial[0]*self.xm_per_pix/self.ym_per_pix**2\n",
    "        Bm=polynomial[1]*self.xm_per_pix/self.ym_per_pix\n",
    "        Cm=polynomial[2]*self.xm_per_pix\n",
    "        newPoli=np.float32([[Am,Bm,Cm]])\n",
    "        curvature = (1+(2*Am*y_eval+Bm)**2)**(3/2)/np.abs(2*Am) \n",
    "        position=Am * (y_eval**2)+Bm*y_eval+Cm\n",
    "        return curvature,position,newPoli\n",
    "    \n",
    "```\n",
    "The Line class also has a method to determine the position of the vehicle with respect to the center of the line, to use that, you may assume the camera is mounted at the center of the car (half_x) and the deviation of the midpoint of the lane from the center of the image is the offset you're looking for. As with the polynomial fitting, convert from pixels to meters. \n",
    "``` python\n",
    " \n",
    "    def get_position_delta(self,l_pos,r_pos)\n",
    "        half_x=self.xsize*self.xm_per_pix/2.0\n",
    "        return ((l_pos+r_pos)/2.0)-half_x\n",
    "```\n",
    "The result of every frame is shown in the output video\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.\n",
    "\n",
    "I implemented this step inside the __call__ method of the class ProcessImage defined in the notebook, to do that, I filled the region between the left and right line pixels in the warped image, and then, I used the inverse_transform_matrix to \"warp back\" the warped region to the original frame perspective, finally, the new_warp (green color region) and the original frame where added using the cv2.addWeighted() function\n",
    "\n",
    "``` python \n",
    "warp_zero = np.zeros((frame.shape[0], frame.shape[1])).astype(np.uint8)\n",
    "color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "pts_left = np.array([np.transpose(np.vstack([self.left_line.allx, self.left_line.ally]))])\n",
    "pts_right = np.array([np.flipud(np.transpose(np.vstack([self.right_line.allx, self.right_line.ally])))])\n",
    "pts = np.hstack((pts_left, pts_right))\n",
    "cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "newwarp = cv2.warpPerspective(color_warp, inverse_transform_matrix, (frame.shape[1], frame.shape[0])) \n",
    "\n",
    "result = cv2.addWeighted(undist, 1, newwarp, 0.3, 0)\n",
    "\n",
    "```\n",
    "![alt text](./safe_region/safe5.png \"safe region image\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline (video)\n",
    "\n",
    "#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).\n",
    "\n",
    "Here's a [link to my video result](./output_video_full_pipeline_reviewed.mp4)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aditional information\n",
    "\n",
    "1- The line class defined in the notebook computes the median value of each coefficient of the polynomial in the last 10 good frames using the method average_last_n()\n",
    "\n",
    "``` python \n",
    "def average_last_n(self,n=10):\n",
    "        if len(self.pixels_polynomial)>n:\n",
    "            averaged_poly=np.mean(self.pixels_polynomial[-n:],axis=0)\n",
    "            median_poly = np.median(self.pixels_polynomial[-n:],axis=0)\n",
    "\n",
    "            self.averaged=True\n",
    "            self.averaged_x = averaged_poly[0]*self.ally**2 + averaged_poly[1]*self.ally + averaged_poly[2]\n",
    "            self.median_x = median_poly[0]*self.ally**2 + median_poly[1]*self.ally + median_poly[2] \n",
    "```\n",
    "2- That class also rejectes frames where the lines are not found in a reasonable position:\n",
    "``` python \n",
    "def check_lines_position(self,left_line_pos,right_line_pos):\n",
    "        # it returns true if the lines are good\n",
    "        lines_distance=right_line_pos-left_line_pos\n",
    "        ## first condition means:\n",
    "            #it rejectes a pair of lines if they are too close or too far away\n",
    "        ## second means:\n",
    "            # if the lines are not in the half that they belongs to\n",
    "        half_x=self.xsize*self.xm_per_pix/2.0\n",
    "        if (lines_distance< self.road_width*0.35 or lines_distance> self.road_width*0.8) or (half_x<left_line_pos or half_x>right_line_pos) : \n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "I think that the pipeline may fail, for example when the camera is under some shadows because the line detection is not really robust. To improve this problem I would create a color adaptative filter, to specifically look for yellow or white in the regions that these colors have a peak in the histogram.\n",
    "\n",
    "I also think that the lines are not soft enough, another filters including the differences in the position of the lines of the frames may be included to avoid the wobbly lines.\n",
    "\n",
    "In the future I would try a supervised learning approach to get the lane lines, training the classifier with pictures and information of the neighborhood of each image.\n",
    "\n",
    "Would be useful to include a method to compare the parallelism of the left and right lines even though they are 2-degree polynomials.\n",
    "\n",
    "The warp fails if the perpendicular of the camera is not aligned with the road.\n",
    "\n",
    "If there are vertical lines or defects near the vertical in the road, the algoritm could missclassify these defects as lane lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
